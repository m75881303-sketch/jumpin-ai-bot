import os
import io
import time
import threading
import logging
from typing import Dict, Tuple

import requests
from flask import Flask

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.constants import ChatAction
from telegram.ext import (
    Application,
    CommandHandler,
    CallbackQueryHandler,
    MessageHandler,
    ContextTypes,
    filters,
)

# -----------------------------
# LOGGING
# -----------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
)
log = logging.getLogger("jump-bot")

# -----------------------------
# ENV
# -----------------------------
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN", "").strip()
HF_TOKEN = os.getenv("HF_TOKEN", "").strip()  # Hugging Face access token (read)

if not TELEGRAM_TOKEN:
    raise RuntimeError("‚ùå –ù–µ—Ç TELEGRAM_TOKEN –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è Render")

# -----------------------------
# FLASK (healthcheck for Render)
# -----------------------------
app = Flask(__name__)

@app.get("/")
def root():
    return "ok", 200

@app.get("/healthz")
def healthz():
    return "ok", 200

def run_flask():
    port = int(os.getenv("PORT", "10000"))
    # –≤–∞–∂–Ω–æ: use_reloader=False, –∏–Ω–∞—á–µ –±—É–¥–µ—Ç 2 –ø—Ä–æ—Ü–µ—Å—Å–∞ -> –∫–æ–Ω—Ñ–ª–∏–∫—Ç –±–æ—Ç–∞
    app.run(host="0.0.0.0", port=port, debug=False, use_reloader=False)

# -----------------------------
# UI / MENU STRUCTURE
# start -> language -> main -> design -> hf -> size -> prompt loop
# -----------------------------
LANGS = [
    ("ru", "–†—É—Å—Å–∫–∏–π"),
    ("en", "English"),
]

# –ü–∏—à–∏ —Å—é–¥–∞ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ HF Inference Router.
# –ï—Å–ª–∏ –±—É–¥–µ—Ç 404 ‚Äî –∑–Ω–∞—á–∏—Ç –º–æ–¥–µ–ª—å/–ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ hf-inference router.
HF_MODELS: Dict[str, str] = {
    "SDXL (stabilityai)": "stabilityai/stable-diffusion-xl-base-1.0",
    "SD v1.5 (runwayml)": "runwayml/stable-diffusion-v1-5",
    # –ï—Å–ª–∏ —Ö–æ—á–µ—à—å FLUX ‚Äî —á–∞—Å—Ç–æ –æ–Ω –ù–ï –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ hf-inference router => –±—É–¥–µ—Ç 404.
    # "FLUX schnell": "black-forest-labs/FLUX.1-schnell",
}

ASPECTS: Dict[str, Tuple[int, int]] = {
    "1:1": (1024, 1024),
    "9:16": (768, 1365),
    "16:9": (1365, 768),
}

K_LANG = "lang"
K_MODEL = "hf_model"
K_ASPECT = "aspect"
K_EXPECT_PROMPT = "expect_prompt"

def kb_language():
    return InlineKeyboardMarkup(
        [[InlineKeyboardButton(title, callback_data=f"lang:{code}")]
         for code, title in LANGS]
    )

def kb_main():
    return InlineKeyboardMarkup([
        [InlineKeyboardButton("üé® –î–∏–∑–∞–π–Ω —Å –ò–ò", callback_data="main:design")],
    ])

def kb_design():
    return InlineKeyboardMarkup([
        [InlineKeyboardButton("ü§ó Hugging Face", callback_data="design:hf")],
        [InlineKeyboardButton("‚¨ÖÔ∏è –ù–∞–∑–∞–¥", callback_data="back:main")],
    ])

def kb_hf_models():
    rows = []
    for title, model_id in HF_MODELS.items():
        rows.append([InlineKeyboardButton(title, callback_data=f"hfmodel:{model_id}")])
    rows.append([InlineKeyboardButton("‚¨ÖÔ∏è –ù–∞–∑–∞–¥", callback_data="back:design")])
    return InlineKeyboardMarkup(rows)

def kb_sizes():
    rows = [[InlineKeyboardButton(a, callback_data=f"size:{a}")] for a in ASPECTS.keys()]
    rows.append([InlineKeyboardButton("‚¨ÖÔ∏è –ù–∞–∑–∞–¥", callback_data="back:hf")])
    return InlineKeyboardMarkup(rows)

def kb_after_prompt():
    return InlineKeyboardMarkup([
        [InlineKeyboardButton("üìê –†–∞–∑–º–µ—Ä", callback_data="menu:size"),
         InlineKeyboardButton("‚¨ÖÔ∏è –í –º–µ–Ω—é", callback_data="menu:main")],
    ])

# -----------------------------
# HuggingFace call (router)
# -----------------------------
def hf_generate_image(model_id: str, prompt: str, width: int, height: int) -> bytes:
    """
    Hugging Face Inference Router.
    –í–∞–∂–Ω–æ: —Ç–æ–∫–µ–Ω –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ Inference Providers.
    """
    if not HF_TOKEN:
        raise RuntimeError("HF_TOKEN –Ω–µ –∑–∞–¥–∞–Ω. –î–æ–±–∞–≤—å –µ–≥–æ –≤ Render ‚Üí Environment.")

    model_id = (model_id or "").strip()
    if not model_id:
        raise RuntimeError("–ü—É—Å—Ç–æ–π model_id")

    url = f"https://router.huggingface.co/hf-inference/models/{model_id}"
    headers = {
        "Authorization": f"Bearer {HF_TOKEN}",
        "Accept": "image/png",
    }

    payload = {
        "inputs": prompt,
        "parameters": {"width": int(width), "height": int(height)},
    }

    r = requests.post(url, headers=headers, json=payload, timeout=180)

    if r.status_code == 200:
        return r.content

    # –ø–æ–∫–∞–∑–∞—Ç—å –ø–æ–Ω—è—Ç–Ω—É—é –æ—à–∏–±–∫—É
    try:
        err = r.json()
    except Exception:
        err = {"error": r.text[:500]}

    raise RuntimeError(f"HF error {r.status_code}: {err}")

# -----------------------------
# Telegram Handlers
# -----------------------------
async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data[K_EXPECT_PROMPT] = False
    await update.message.reply_text("–í—ã–±–µ—Ä–∏ —è–∑—ã–∫ üëá", reply_markup=kb_language())

async def cmd_menu(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data[K_EXPECT_PROMPT] = False
    await update.message.reply_text("–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é üëá", reply_markup=kb_main())

async def on_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    q = update.callback_query
    await q.answer()
    data = q.data or ""

    if data.startswith("lang:"):
        context.user_data[K_LANG] = data.split(":", 1)[1]
        context.user_data[K_EXPECT_PROMPT] = False
        await q.edit_message_text("–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é üëá", reply_markup=kb_main())
        return

    if data == "main:design":
        context.user_data[K_EXPECT_PROMPT] = False
        await q.edit_message_text("üé® –î–∏–∑–∞–π–Ω —Å –ò–ò ‚Äî –≤—ã–±–µ—Ä–∏ –∏—Å—Ç–æ—á–Ω–∏–∫:", reply_markup=kb_design())
        return

    if data == "design:hf":
        context.user_data[K_EXPECT_PROMPT] = False
        await q.edit_message_text("ü§ó Hugging Face ‚Äî –≤—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å:", reply_markup=kb_hf_models())
        return

    if data.startswith("hfmodel:"):
        model_id = data.split(":", 1)[1].strip()
        context.user_data[K_MODEL] = model_id
        context.user_data[K_EXPECT_PROMPT] = False
        await q.edit_message_text("–í—ã–±–µ—Ä–∏ —Ä–∞–∑–º–µ—Ä (—Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ç–æ—Ä–æ–Ω):", reply_markup=kb_sizes())
        return

    if data.startswith("size:"):
        aspect = data.split(":", 1)[1].strip()
        context.user_data[K_ASPECT] = aspect
        context.user_data[K_EXPECT_PROMPT] = True

        model_id = context.user_data.get(K_MODEL, "")
        await q.edit_message_text(
            "‚úçÔ∏è –û—Ç–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞.\n\n"
            f"–ú–æ–¥–µ–ª—å: {model_id}\n"
            f"–†–∞–∑–º–µ—Ä: {aspect}\n\n"
            "–ü–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç–æ –ø–∏—à–∏ —Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–º–ø—Ç ‚Äî /start –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω.",
            reply_markup=kb_after_prompt(),
        )
        return

    if data == "menu:size":
        await q.edit_message_text("–í—ã–±–µ—Ä–∏ —Ä–∞–∑–º–µ—Ä:", reply_markup=kb_sizes())
        return

    if data == "menu:main":
        context.user_data[K_EXPECT_PROMPT] = False
        await q.edit_message_text("–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é üëá", reply_markup=kb_main())
        return

    if data == "back:main":
        context.user_data[K_EXPECT_PROMPT] = False
        await q.edit_message_text("–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é üëá", reply_markup=kb_main())
        return

    if data == "back:design":
        context.user_data[K_EXPECT_PROMPT] = False
        await q.edit_message_text("üé® –î–∏–∑–∞–π–Ω —Å –ò–ò ‚Äî –≤—ã–±–µ—Ä–∏ –∏—Å—Ç–æ—á–Ω–∏–∫:", reply_markup=kb_design())
        return

    if data == "back:hf":
        context.user_data[K_EXPECT_PROMPT] = False
        await q.edit_message_text("ü§ó Hugging Face ‚Äî –≤—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å:", reply_markup=kb_hf_models())
        return

    await q.edit_message_text("–ù–µ –ø–æ–Ω—è–ª–∞ –¥–µ–π—Å—Ç–≤–∏–µ. –ù–∞–∂–º–∏ /menu", reply_markup=kb_main())

async def on_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = (update.message.text or "").strip()
    if not text:
        return

    if not context.user_data.get(K_EXPECT_PROMPT, False):
        await update.message.reply_text("–û—Ç–∫—Ä–æ–π –º–µ–Ω—é üëá", reply_markup=kb_main())
        return

    model_id = (context.user_data.get(K_MODEL) or "").strip()
    aspect = (context.user_data.get(K_ASPECT) or "1:1").strip()

    if not model_id:
        context.user_data[K_EXPECT_PROMPT] = False
        await update.message.reply_text("–°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å –≤ –º–µ–Ω—é üëá", reply_markup=kb_main())
        return

    w, h = ASPECTS.get(aspect, (1024, 1024))

    try:
        await update.message.chat.send_action(ChatAction.UPLOAD_PHOTO)

        # –±–ª–æ–∫–∏—Ä—É—é—â–∏–π requests -> –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫
        img_bytes = await asyncio_to_thread(hf_generate_image, model_id, text, w, h)

        bio = io.BytesIO(img_bytes)
        bio.name = "image.png"
        bio.seek(0)

        await update.message.reply_photo(photo=bio, caption="‚úÖ –ì–æ—Ç–æ–≤–æ!", reply_markup=kb_after_prompt())
        context.user_data[K_EXPECT_PROMPT] = True

    except Exception as e:
        log.exception("Generation failed")
        await update.message.reply_text(
            "üòï –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:\n"
            f"{e}\n\n"
            "–ú–æ–∂–µ—à—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –¥—Ä—É–≥–æ–π –ø—Ä–æ–º–ø—Ç –∏–ª–∏ –ø–æ–º–µ–Ω—è—Ç—å —Ä–∞–∑–º–µ—Ä/–º–æ–¥–µ–ª—å.",
            reply_markup=kb_after_prompt(),
        )
        context.user_data[K_EXPECT_PROMPT] = True

# ---- –º–∞–ª–µ–Ω—å–∫–∏–π async-to-thread –±–µ–∑ –ø—Ä–æ–±–ª–µ–º –Ω–∞ 3.13
async def asyncio_to_thread(func, *args, **kwargs):
    result = []
    exc = []

    def runner():
        try:
            result.append(func(*args, **kwargs))
        except Exception as e:
            exc.append(e)

    t = threading.Thread(target=runner, daemon=True)
    t.start()
    while t.is_alive():
        time.sleep(0.05)

    if exc:
        raise exc[0]
    return result[0]

# -----------------------------
# Telegram Runner (polling)
# -----------------------------
def run_telegram_polling():
    if not TELEGRAM_TOKEN:
        log.error("‚ùå TELEGRAM_TOKEN missing")
        return

    log.info("‚úÖ Starting Telegram polling...")

    application = Application.builder().token(TELEGRAM_TOKEN).build()

    application.add_handler(CommandHandler("start", cmd_start))
    application.add_handler(CommandHandler("menu", cmd_menu))
    application.add_handler(CallbackQueryHandler(on_callback))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, on_text))

    # –í–∞–∂–Ω–æ: –µ—Å–ª–∏ –≥–¥–µ-—Ç–æ –±—ã–ª webhook ‚Äî polling –Ω–µ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å.
    # PTB —Å–∞–º –¥–µ—Ä–≥–∞–µ—Ç deleteWebhook –≤–Ω—É—Ç—Ä–∏ run_polling, –Ω–æ –æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –µ—Å—Ç—å.
    application.run_polling(drop_pending_updates=# -----------------------------
# MAIN
# -----------------------------
if __name__ == "__main__":
    # 1) Flask –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ (healthcheck –¥–ª—è Render)
    threading.Thread(target=run_flask, daemon=True).start()

    # 2) Telegram polling ‚Äî –í –ì–õ–ê–í–ù–û–ú –ü–û–¢–û–ö–ï
    log.info("‚úÖ Starting Telegram polling in main thread...")
    run_telegram_polling()
